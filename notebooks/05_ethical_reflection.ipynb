{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "05_ethics_and_limitations.ipynb\n",
        "\n",
        "#Week 5 - Ethical Reflection\n",
        "\n",
        "\n",
        "###Ethical Concerns in Anomaly Detection\n",
        "\n",
        "\n",
        "When applying anomaly detection algorithms, especially in sensitive domains such as finance or behavioural monitoring, several ethical concerns must be considered:\n",
        "\n",
        "- **Opacity of Black-box Models:** Models such as deep neural networks may lack transparency. As Ludera (2021) notes, conventional classifiers often misrepresent rare events due to imbalance, leading to unjustified decisions in high-stakes contexts.\n",
        "- **Bias in Imbalanced Data:** Both papers highlight the challenge of skewed class distributions (e.g. legitimate vs. fraudulent transactions). Without rebalancing (e.g. SMOTE ENN), models may overlook minority-class behaviours, which could disadvantage vulnerable groups or legitimate outliers.\n",
        "- **Consequences of False Positives:** In both the marketing and fraud detection contexts (Fawei & Ludera, 2020; Ludera, 2021), models must be carefully tuned to avoid penalising legitimate customers based on atypical but harmless behaviour patterns.\n",
        "\n",
        "\n",
        "###False Positives and Data Misinterpretation\n",
        "\n",
        "In practice, false positives may lead to service denial, unnecessary investigations, or reputational harm. For example, a high-spending client flagged as anomalous may simply represent an emerging pattern not present in training data.\n",
        "\n",
        "\n",
        "###Example Scenario\n",
        "\n",
        "\n",
        "Imagine a public services monitoring system where a userâ€™s behaviour (e.g. time-of-day usage or irregular attendance) is flagged as suspicious. If the system is trained on biased or sparse data, this user could be wrongly profiled, impacting their access to services.\n",
        "\n",
        "\n",
        "###Questions for Learners\n",
        "\n",
        "\n",
        "- How should we handle flagged anomalies when no ground truth is available?\n",
        "- Should users be notified if their behaviour is being monitored by automated models?\n",
        "- How can we mitigate harm from false positives without reducing detection sensitivity?\n",
        "\n",
        "\n",
        "###References\n",
        "\n",
        "\n",
        "1. Ludera, D.T.J. (2021) *Credit Card Fraud Detection by Combining Synthetic Minority Oversampling and Edited Nearest Neighbours*. FICC 2021.\n",
        "2. Fawei, T. & Ludera, D.T.J. (2020) *Data Mining Solutions for Direct Marketing Campaigns*. Intelligent Systems and Applications.\n",
        "3. Dua, D. & Graff, C. (2017) *UCI Machine Learning Repository*. [Time Series Anomaly Benchmark].\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRowcmjqbf95"
      }
    }
  ]
}