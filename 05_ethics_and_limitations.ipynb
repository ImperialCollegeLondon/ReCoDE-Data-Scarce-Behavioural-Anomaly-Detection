{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a4143588",
      "metadata": {
        "id": "a4143588"
      },
      "source": [
        "#  05  Ethical Reflection and False Positive Risks\n",
        "\n",
        "This notebook explores key ethical concerns in unsupervised anomaly detection, particularly in behavioural or operational contexts. Learners are encouraged to reflect critically on the social impact, fairness, and responsibility of deploying such models in real-world systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e143706c",
      "metadata": {
        "id": "e143706c"
      },
      "source": [
        "### **Step 1 - Ethical Risks in Anomaly Detection**\n",
        "\n",
        "Opacity in algorithmic reasoning remains a key concern, particularly in domains where decisions carry social or reputational consequences. Black-box models, such as deep neural networks, may yield high-performance outcomes but often lack transparency in how conclusions are drawn. In contexts like finance or behavioural monitoring, this can lead to automated decisions without clear justification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "VIaGhuTBFi_V"
      },
      "id": "VIaGhuTBFi_V"
    },
    {
      "cell_type": "markdown",
      "id": "c1f3c7a8",
      "metadata": {
        "id": "c1f3c7a8"
      },
      "source": [
        "### **Step 2 - Data Bias and Model Fairness**\n",
        "\n",
        "As demonstrated in Ludera (2021), fraud detection datasets are often highly imbalanced, with fraudulent transactions forming only a small minority. Without appropriate preprocessing such as SMOTE-ENN or other class-balancing techniques, machine learning models tend to overfit to the dominant class and may wrongly learn to treat legitimate transactions as anomalous. These misclassifications arise not from genuine behavioural irregularity but from the model's distorted understanding of rarity. If left uncorrected, such hallucinations can result in systemic bias against honest users."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "e4kSGAefFkNN"
      },
      "id": "e4kSGAefFkNN"
    },
    {
      "cell_type": "markdown",
      "id": "a8bc86ce",
      "metadata": {
        "id": "a8bc86ce"
      },
      "source": [
        "### Step 3 - **The Impact of False Positives**\n",
        "\n",
        "As shown in Fawei & Ludera (2020), even in non-sensitive domains such as marketing, misclassification can carry significant consequences. In the direct marketing experiments, the model was trained on a dataset where the majority of customers declined the offer. Without careful calibration, the system risked overlooking genuinely interested customers, simply because their profiles differed from the dominant pattern. By tuning error cost parameters and interpreting the confusion matrix, the study demonstrated how important it is to minimise false rejections, even when ground truth is not uniformly distributed.\n",
        "\n",
        "Similar risks were later explored in Ludera (2021), where false positives in credit card fraud detection could lead to service blocks for legitimate users. This highlights the broader implications of imbalance-related misclassification across domains, particularly where the cost of incorrect labelling is socially or financially significant."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "T8G7qHFLFmPl"
      },
      "id": "T8G7qHFLFmPl"
    },
    {
      "cell_type": "markdown",
      "id": "3da469e4",
      "metadata": {
        "id": "3da469e4"
      },
      "source": [
        "### Step 4 - **Illustrative Scenario**\n",
        "\n",
        "Imagine a public service system where user behaviour, such as late attendance or sporadic access, is monitored for anomalies. A low-income user who works night shifts may present unusual usage patterns. If the model was trained predominantly on daytime users, this behaviour could be wrongly flagged, potentially limiting access to essential services or triggering further surveillance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "rTScthz7FniN"
      },
      "id": "rTScthz7FniN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 - Proposed Mitigations and Safeguards\n",
        "\n",
        "In line with established principles for trustworthy AI, several safeguards may be introduced to reduce the harm caused by automated misclassifications. These include embedding interpretability techniques, applying calibrated anomaly score thresholds, and implementing human fallback review layers.\n",
        "\n",
        "In high-risk domains, models should incorporate audit trails to ensure accountability, and all flagged outputs ought to be subject to proportional, human-led interpretation prior to any consequential action. These recommendations are consistent with international guidelines on AI ethics, which emphasise transparency, oversight, and the prioritisation of human well-being [3], [4]."
      ],
      "metadata": {
        "id": "0ZxajF3fPc3C"
      },
      "id": "0ZxajF3fPc3C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "9ysF61-QPoyl"
      },
      "id": "9ysF61-QPoyl"
    },
    {
      "cell_type": "markdown",
      "id": "5f949d7f",
      "metadata": {
        "id": "5f949d7f"
      },
      "source": [
        "### Step 6 - Learner Reflection Questions\n",
        "\n",
        "- How should anomalies be handled when no ground truth is available?\n",
        "- Should users be informed if their behaviour is being flagged by automated systems?\n",
        "- How can developers mitigate harm from false positives while preserving model sensitivity?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Mt-q3tkUF00t"
      },
      "id": "Mt-q3tkUF00t"
    },
    {
      "cell_type": "markdown",
      "id": "176ae9dc",
      "metadata": {
        "id": "176ae9dc"
      },
      "source": [
        "### **References**\n",
        "\n",
        "1. Ludera, D.T.J. (2021). *Credit Card Fraud Detection by Combining Synthetic Minority Oversampling and Edited Nearest Neighbours*. In: FICC 2021 - Future of Information and Communication Conference.\n",
        "\n",
        "2. Fawei, T. & Ludera, D.T.J. (2020). *Data Mining Solutions for Direct Marketing Campaigns*. Intelligent Systems and Applications.\n",
        "\n",
        "3. European Commission (High-Level Expert Group on Artificial Intelligence) (2019). Ethics Guidelines for Trustworthy Artificial Intelligence. Brussels: European Commission. Available at: https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai (Accessed: 10 July 2025).\n",
        "\n",
        "4. IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (2017). Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (Version 2). New York: Institute of Electrical and Electronics Engineers. Available at: https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf (Accessed: 10 July 2025)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}