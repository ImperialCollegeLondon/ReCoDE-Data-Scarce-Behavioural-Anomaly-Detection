{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#data-scarce-behavioural-anomaly-detection","title":"Data-Scarce Behavioural Anomaly Detection","text":""},{"location":"#overview","title":"Overview","text":"<p>This exemplar provides a complete pipeline for unsupervised anomaly detection applied to univariate time series data. Using the InternalBleeding14 dataset from the UCR Time Series Anomaly Archive, the project demonstrates techniques for detecting irregular patterns in physiological-style sensor recordings, where normal operating conditions are occasionally interrupted by anomalous deviations. The exemplar guides learners through data preparation, preprocessing, Isolation Forest modelling, dimensionality reduction with PCA, clustering with HDBSCAN, model interpretation, and ethical considerations when analysing scarce or sensitive time series data. The exemplar is fully modular, industry-aligned, and reproducible for academic and applied machine learning use cases.</p>"},{"location":"#dataset","title":"Dataset","text":"<ul> <li>Dataset name: InternalBleeding14  </li> <li>Source: UCR Time Series Anomaly Archive  </li> <li>Citation:   \u25aa Wu, R., &amp; Keogh, E. (2020). Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress. arXiv:2009.13807</li> </ul>"},{"location":"#weekly-structure","title":"Weekly Structure","text":"<ul> <li>Week 1: Dataset loading, conversion and justification  </li> <li>Week 2: Preprocessing and baseline Isolation Forest  </li> <li>Week 3: PCA visualisation and HDBSCAN clustering  </li> <li>Week 4: Model interpretation with markdown explanations  </li> <li>Week 5: Ethical reflection on anomalies and data scarcity</li> <li>Week 6: Visual polishing and markdown refinement  </li> <li>Week 7: Environment file and reproducibility tests  </li> <li>Week 8: Final README, dataset framing, and documentation  </li> <li>Week 9: Review and minor corrections  </li> <li>Week 10: Final submission and packaging</li> </ul>"},{"location":"#tools-used","title":"Tools Used","text":"<ul> <li>Python 3.x</li> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>HDBSCAN</li> <li>matplotlib</li> <li>seaborn</li> </ul>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>.\n\u251c\u2500\u2500 notebooks\n\u2502 \u251c\u2500\u2500 01_dataset_preparation.ipynb\n\u2502 \u251c\u2500\u2500 02_preprocessing_and_baseline_iforest.ipynb\n\u2502 \u251c\u2500\u2500 03_dimensionality_and_clustering.ipynb\n\u2502 \u251c\u2500\u2500 04_model_interpretation_and_explanation.ipynb\n\u2502 \u251c\u2500\u2500 05_ethical_reflection.ipynb\n\u2502 \u251c\u2500\u2500 06_visual_polishing_and_citations.ipynb\n\u2502 \u251c\u2500\u2500 07_reproducibility_and_environment_testing.ipynb\n\u2502 \u2514\u2500\u2500 08_finalised_summary_notebook.ipynb\n\u251c\u2500\u2500 data\n\u2502 \u2514\u2500\u2500 InternalBleeding14.csv\n\u251c\u2500\u2500 src\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 utils\n\u251c\u2500\u2500 test\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 .github/workflows\n</code></pre>"},{"location":"#licensing","title":"Licensing","text":"<p>BSD-3-Clause License</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This exemplar was developed at Imperial College London by Duke T. J. Ludera, in collaboration with Saranjeet Kaur S. S. Bhogal from Research Software Engineering, and Dr Jianliang Gao from Research Computing &amp; Data Science at the Early Career Researcher Institute.</p>"},{"location":"01_dataset_preparation/","title":"01 Dataset Preparation","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns from sklearn.preprocessing import MinMaxScaler In\u00a0[\u00a0]: Copied! <pre>file_path = 'data/InternalBleeding14.csv'\n</pre> file_path = 'data/InternalBleeding14.csv' In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(file_path, sep=\",\")\n</pre> df = pd.read_csv(file_path, sep=\",\") In\u00a0[\u00a0]: Copied! <pre>print(\"\\nFirst 10 rows of dataset:\")\ndisplay(df.head(10))\n</pre> print(\"\\nFirst 10 rows of dataset:\") display(df.head(10)) <pre>\nFirst 10 rows of dataset:\n</pre> timestamp value 0 0 97.46170 1 1 97.38159 2 2 97.18323 3 3 96.96197 4 4 96.67206 5 5 96.55380 6 6 96.62247 7 7 96.48132 8 8 96.52329 9 9 96.79413 In\u00a0[\u00a0]: Copied! <pre>print(\"\\nDataset information:\")\ndf.info()\n</pre> print(\"\\nDataset information:\") df.info() <pre>\nDataset information:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7501 entries, 0 to 7500\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   timestamp  7501 non-null   int64  \n 1   value      7501 non-null   float64\ndtypes: float64(1), int64(1)\nmemory usage: 117.3 KB\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n</pre> print(\"\\nMissing values per column:\") print(df.isnull().sum()) <pre>\nMissing values per column:\ntimestamp    0\nvalue        0\ndtype: int64\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\\nBasic descriptive statistics:\")\ndisplay(df.describe())\n</pre> print(\"\\nBasic descriptive statistics:\") display(df.describe()) <pre>\nBasic descriptive statistics:\n</pre> timestamp value count 7501.000000 7501.000000 mean 3750.000000 84.835175 std 2165.496517 10.459530 min 0.000000 66.604610 25% 1875.000000 75.939180 50% 3750.000000 83.328250 75% 5625.000000 93.044280 max 7500.000000 108.680700 In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(14, 6))\nplt.plot(df['timestamp'], df['value'], color='steelblue')\nplt.title('InternalBleeding14 - Full Time Series')\nplt.xlabel('Timestamp')\nplt.ylabel('Value')\nplt.grid()\nplt.show()\n</pre> plt.figure(figsize=(14, 6)) plt.plot(df['timestamp'], df['value'], color='steelblue') plt.title('InternalBleeding14 - Full Time Series') plt.xlabel('Timestamp') plt.ylabel('Value') plt.grid() plt.show()"},{"location":"01_dataset_preparation/#01-dataset-preparation","title":"01 Dataset Preparation\u00b6","text":"<p>This notebook begins the Week 1 task of verifying the structure and completeness of the dataset. We shall preview the data, inspect column integrity, and prepare it for anomaly detection.</p>"},{"location":"01_dataset_preparation/#step-1-import-required-libraries","title":"Step 1 - Import Required Libraries\u00b6","text":"<p>We begin by importing the necessary libraries for data handling, visualisation, and modelling. These libraries provide the core tools required for preparing the data, creating plots, scaling values:</p> <ul> <li><p>pandas - used for loading, organising, and manipulating the dataset in tabular form.</p> </li> <li><p>numpy - provides support for efficient numerical operations and array handling.</p> </li> <li><p>matplotlib.pyplot - enables creation of static plots for visualising time series data and results.</p> </li> <li><p>seaborn - enhances visualisation with more attractive and informative statistical plots.</p> </li> <li><p>MinMaxScaler (from sklearn.preprocessing) - scales features to a defined range (0 to 1 in our case), essential for distance-based models.</p> </li> </ul>"},{"location":"01_dataset_preparation/#step-2-define-file-path","title":"Step 2 - Define File Path\u00b6","text":"<p>We define the relative path to the dataset. This assumes that <code>InternalBleeding14.csv</code> is stored in the <code>data/</code> subfolder.</p>"},{"location":"01_dataset_preparation/#step-3-load-dataset","title":"Step 3 - Load Dataset\u00b6","text":"<p>We load the dataset into a pandas DataFrame for further exploration and processing.</p>"},{"location":"01_dataset_preparation/#step-4-preview-the-dataset","title":"Step 4 - Preview the Dataset\u00b6","text":"<p>We print the first 10 rows to check that the data has loaded correctly and to confirm that the <code>timestamp</code> and <code>value</code> columns are present and well-formed.</p>"},{"location":"01_dataset_preparation/#observations-from-the-preview","title":"Observations from the Preview\u00b6","text":"<p>The <code>timestamp</code> column begins at 0 and increments sequentially, suggesting a regular time index. The <code>value</code> column contains floating-point sensor readings with no obvious irregularities or missing values. These entries suggest that the dataset is correctly structured and ready for further inspection.</p>"},{"location":"01_dataset_preparation/#initial-data-inspection","title":"Initial Data Inspection\u00b6","text":""},{"location":"01_dataset_preparation/#step-5-dataset-information","title":"Step 5 - Dataset Information\u00b6","text":"<p>We first inspect the dataset\u2019s structure using <code>.info()</code>. This confirms the column types, memory usage, and whether any fields are missing or malformed.</p>"},{"location":"01_dataset_preparation/#interpreting-the-dataset-structure","title":"Interpreting the Dataset Structure\u00b6","text":"<p>The dataset contains 7501 rows and 2 columns:</p> <ul> <li><code>timestamp</code>: integer type, acting as sequential time index.</li> <li><code>value</code>: float type, representing the observed signal measurement.</li> </ul> <p>All entries are non-null, indicating no missing data in either column.</p>"},{"location":"01_dataset_preparation/#step-6-check-for-missing-values","title":"Step 6 - Check for Missing Values\u00b6","text":"<p>We verify that there are no missing values in either column. This ensures data completeness and avoids preprocessing issues later.</p>"},{"location":"01_dataset_preparation/#interpreting-missing-value-report","title":"Interpreting Missing Value Report\u00b6","text":"<p>Both columns show zero missing values, confirming dataset completeness prior to analysis.</p>"},{"location":"01_dataset_preparation/#step-7-descriptive-statistics","title":"Step 7 -  Descriptive Statistics\u00b6","text":"<p>Basic statistics such as mean, standard deviation, min, and max help us understand the overall distribution and variability of the signal.</p>"},{"location":"01_dataset_preparation/#summary-of-statistical-characteristics","title":"Summary of Statistical Characteristics\u00b6","text":"<ul> <li>Count: Confirms full data length (7501 entries)</li> <li>Mean: ~84.83 indicates the central tendency</li> <li>Std: ~10.46 shows moderate variability</li> <li>Min/Max: Range between 66.60 and 108.68 suggests possible outlier zones</li> <li>Quartiles: Reveal spread and central concentration</li> </ul> <p>Educational Note: These stats help characterise what \u2018normal\u2019 looks like in the signal. Useful later for anomaly thresholds.</p>"},{"location":"01_dataset_preparation/#step-8-full-time-series-plot","title":"Step 8 - Full Time Series Plot\u00b6","text":"<p>This plot allows visual inspection of the signal\u2019s behaviour. We observe a clear periodic structure with variations in amplitude, which may contain anomalies.</p>"},{"location":"01_dataset_preparation/#observations-and-interpretation","title":"Observations and Interpretation\u00b6","text":"<p>The time series shows a consistent cyclic structure, typical of physiological sensor signals.</p> <p>Key observations:</p> <ul> <li>Stable periodicity across duration</li> <li>Amplitude ranges from ~66 to ~108</li> <li>Sharp peaks and troughs may indicate anomalous events</li> </ul> <p>Interpretation:</p> <p>The signal displays a regular periodic structure with clear oscillations and stable frequency. Amplitude varies between approximately 66 and 108 units, with occasional sharp deviations that may correspond to anomalous events.</p> <p>Educational Note: This visual step grounds the modeller in the signal\u2019s dynamics before applying any detection algorithms.</p>"},{"location":"01_dataset_preparation/#educational-note","title":"Educational Note:\u00b6","text":"<p>These descriptive stats are crucial for understanding normal behaviour and early suspicion of potential anomalies. The full amplitude range suggests physiological fluctuation typical for this benchmark dataset.</p>"},{"location":"02_preprocessing_and_baseline_iforest/","title":"02 Baseline Anomaly Detection","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import IsolationForest\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns from sklearn.preprocessing import MinMaxScaler from sklearn.ensemble import IsolationForest In\u00a0[\u00a0]: Copied! <pre>file_path = 'data/InternalBleeding14.csv'\n</pre> file_path = 'data/InternalBleeding14.csv' In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(file_path, sep=\",\")\n</pre> df = pd.read_csv(file_path, sep=\",\") In\u00a0[\u00a0]: Copied! <pre>print(\"\\nFirst 10 rows of dataset:\")\ndisplay(df.head(10))\n</pre> print(\"\\nFirst 10 rows of dataset:\") display(df.head(10)) <pre>\nFirst 10 rows of dataset:\n</pre> timestamp value 0 0 97.46170 1 1 97.38159 2 2 97.18323 3 3 96.96197 4 4 96.67206 5 5 96.55380 6 6 96.62247 7 7 96.48132 8 8 96.52329 9 9 96.79413 In\u00a0[\u00a0]: Copied! <pre>scaler = MinMaxScaler()\ndf['value_scaled'] = scaler.fit_transform(df[['value']])\n</pre> scaler = MinMaxScaler() df['value_scaled'] = scaler.fit_transform(df[['value']]) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nFirst 10 scaled values:\")\nprint(df[['value', 'value_scaled']].head(10))\n</pre> print(\"\\nFirst 10 scaled values:\") print(df[['value', 'value_scaled']].head(10)) <pre>\nFirst 10 scaled values:\n      value  value_scaled\n0  97.46170      0.733364\n1  97.38159      0.731460\n2  97.18323      0.726746\n3  96.96197      0.721487\n4  96.67206      0.714597\n5  96.55380      0.711786\n6  96.62247      0.713418\n7  96.48132      0.710064\n8  96.52329      0.711061\n9  96.79413      0.717498\n</pre> In\u00a0[\u00a0]: Copied! <pre>isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\ndf['anomaly_score'] = isolation_forest.fit_predict(df[['value_scaled']])\n</pre> isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42) df['anomaly_score'] = isolation_forest.fit_predict(df[['value_scaled']]) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nFirst 10 rows with anomaly scores:\")\nprint(df[['value', 'value_scaled', 'anomaly_score']].head(10))\n</pre> print(\"\\nFirst 10 rows with anomaly scores:\") print(df[['value', 'value_scaled', 'anomaly_score']].head(10)) <pre>\nFirst 10 rows with anomaly scores:\n      value  value_scaled  anomaly_score\n0  97.46170      0.733364              1\n1  97.38159      0.731460              1\n2  97.18323      0.726746              1\n3  96.96197      0.721487              1\n4  96.67206      0.714597              1\n5  96.55380      0.711786              1\n6  96.62247      0.713418              1\n7  96.48132      0.710064              1\n8  96.52329      0.711061              1\n9  96.79413      0.717498              1\n</pre> <p>Observations from Anomaly Scores</p> <p>The first 10 rows all show an anomaly_score of 1, indicating that Isolation Forest has classified these points as normal. The model assigns 1 for inliers (normal points) and -1 for outliers (potential anomalies).</p> In\u00a0[\u00a0]: Copied! <pre>df['anomaly'] = df['anomaly_score'].map({1: 0, -1: 1})\n</pre> df['anomaly'] = df['anomaly_score'].map({1: 0, -1: 1}) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nSample of dataset with anomaly predictions:\")\ndisplay(df.head(10))\n</pre> print(\"\\nSample of dataset with anomaly predictions:\") display(df.head(10)) <pre>\nSample of dataset with anomaly predictions:\n</pre> timestamp value value_scaled anomaly_score anomaly 0 0 97.46170 0.733364 1 0 1 1 97.38159 0.731460 1 0 2 2 97.18323 0.726746 1 0 3 3 96.96197 0.721487 1 0 4 4 96.67206 0.714597 1 0 5 5 96.55380 0.711786 1 0 6 6 96.62247 0.713418 1 0 7 7 96.48132 0.710064 1 0 8 8 96.52329 0.711061 1 0 9 9 96.79413 0.717498 1 0 In\u00a0[\u00a0]: Copied! <pre>print(\"\\nAnomaly counts:\")\ndisplay(df['anomaly'].value_counts())\n</pre> print(\"\\nAnomaly counts:\") display(df['anomaly'].value_counts()) <pre>\nAnomaly counts:\n</pre> count anomaly 0 7127 1 374 dtype: int64 In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16,6))\nplt.plot(df['timestamp'], df['value'], label='Original Signal', color='steelblue')\nplt.title('Original Signal')\nplt.xlabel('Timestamp')\nplt.ylabel('Value')\nplt.grid()\nplt.show()\n</pre> plt.figure(figsize=(16,6)) plt.plot(df['timestamp'], df['value'], label='Original Signal', color='steelblue') plt.title('Original Signal') plt.xlabel('Timestamp') plt.ylabel('Value') plt.grid() plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16,6))\nplt.plot(df['timestamp'], df['value'], label='Signal', color='lightgray')\n\nplt.scatter(\n    df[df['anomaly'] == 1]['timestamp'],\n    df[df['anomaly'] == 1]['value'],\n    color='red',\n    label='Anomalies',\n    marker='x',\n    s=60\n)\n\nplt.title('Anomaly Detection using Isolation Forest')\nplt.xlabel('Timestamp')\nplt.ylabel('Value')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> plt.figure(figsize=(16,6)) plt.plot(df['timestamp'], df['value'], label='Signal', color='lightgray')  plt.scatter(     df[df['anomaly'] == 1]['timestamp'],     df[df['anomaly'] == 1]['value'],     color='red',     label='Anomalies',     marker='x',     s=60 )  plt.title('Anomaly Detection using Isolation Forest') plt.xlabel('Timestamp') plt.ylabel('Value') plt.legend() plt.grid() plt.show()  In\u00a0[\u00a0]: Copied! <pre>file_path = 'data/209Fantasia_clean.csv'\n</pre> file_path = 'data/209Fantasia_clean.csv'"},{"location":"02_preprocessing_and_baseline_iforest/#02-baseline-anomaly-detection","title":"02 Baseline Anomaly Detection\u00b6","text":"<p>This notebook begins the Week 2 task of applying preprocessing, run Isolation Forest, basic anomaly detection.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#step-1-import-required-libraries","title":"Step 1 - Import Required Libraries\u00b6","text":"<p>We begin by importing the necessary libraries for data handling, visualisation, and modelling. These libraries provide the core tools required for preparing the data, creating plots, scaling values, and detecting anomalies:</p> <ul> <li><p>pandas - used for loading, organising, and manipulating the dataset in tabular form.</p> </li> <li><p>numpy - provides support for efficient numerical operations and array handling.</p> </li> <li><p>matplotlib.pyplot - enables creation of static plots for visualising time series data and results.</p> </li> <li><p>seaborn - enhances visualisation with more attractive and informative statistical plots.</p> </li> <li><p>MinMaxScaler (from sklearn.preprocessing) - scales features to a defined range (0 to 1 in our case), essential for distance-based models.</p> </li> <li><p>IsolationForest (from sklearn.ensemble) - implements the Isolation Forest algorithm, used here for unsupervised anomaly detection.</p> </li> </ul>"},{"location":"02_preprocessing_and_baseline_iforest/#step-2-define-file-path","title":"Step 2 - Define File Path\u00b6","text":"<p>We define the relative path to the dataset. This assumes that <code>InternalBleeding14.csv</code> is stored in the <code>data/</code> subfolder.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#step-3-load-dataset","title":"Step 3 - Load Dataset\u00b6","text":"<p>We load the dataset into a pandas DataFrame for further exploration and processing.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#step-4-preview-the-dataset","title":"Step 4 - Preview the Dataset\u00b6","text":"<p>We print the first 10 rows to check that the data has loaded correctly and to confirm that the <code>timestamp</code> and <code>value</code> columns are present and well-formed.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#observations-from-the-preview","title":"Observations from the Preview\u00b6","text":"<p>The <code>timestamp</code> column begins at 0 and increments sequentially, suggesting a regular time index. The <code>value</code> column contains floating-point sensor readings with no obvious irregularities or missing values. These entries suggest that the dataset is correctly structured and ready for further inspection.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#step-5-normalise-value-column-min-max-scaling","title":"Step 5 - Normalise value column (Min-Max Scaling)\u00b6","text":"<p>We apply Min-Max scaling to normalise the <code>value</code> column between 0 and 1. This ensures that features are on the same scale, which is important for distance-based models like clustering and anomaly detection.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#display-first-few-scaled-values-for-confirmation","title":"Display first few scaled values for confirmation\u00b6","text":""},{"location":"02_preprocessing_and_baseline_iforest/#observations-from-scaled-values","title":"Observations from Scaled Values\u00b6","text":"<p>The value_scaled column shows that the original signal values have been successfully transformed to a 0\u20131 range. Higher original values correspond to scaled values closer to 1, and lower original values correspond to values nearer 0, as expected from Min-Max scaling.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#step-6-apply-isolation-forest-for-anomaly-detection","title":"Step 6 - Apply Isolation Forest for anomaly detection\u00b6","text":"<p>In this step, we use Isolation Forest to identify potential anomalies in the scaled signal. Isolation Forest is an unsupervised model designed to detect outliers by isolating data points that behave differently from the majority.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#define-model","title":"Define model\u00b6","text":"<p>We define the Isolation Forest model with 100 trees (<code>n_estimators=100</code>) and set the expected proportion of anomalies to 5% (<code>contamination=0.05</code>). A fixed <code>random_state</code> ensures reproducibility of the results.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#display-the-first-10-rows-with-anomaly-scores","title":"Display the first 10 rows with anomaly scores\u00b6","text":"<p>This output will show the original value, its scaled version, and whether the model classified it as normal (1) or anomalous (-1).</p>"},{"location":"02_preprocessing_and_baseline_iforest/#step-7-convert-anomaly-output","title":"Step 7 - Convert anomaly output\u00b6","text":"<p>We convert the model's anomaly score to a clearer format for analysis and plotting. Here, <code>0</code> represents normal points and <code>1</code> represents anomalies.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#display-small-sample-for-verification","title":"Display small sample for verification\u00b6","text":"<p>We display the first 10 rows of the dataset to verify that anomaly labels have been added correctly alongside the original values and scaled data.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#observations-from-sample-output","title":"Observations from Sample Output\u00b6","text":"<p>The sample confirms that the anomaly column has been added correctly. All shown points are classified as normal (0), consistent with their anomaly_score of 1.</p> <ul> <li>We display the first 10 rows after applying Isolation Forest.</li> <li>New columns:<ul> <li><code>value_scaled</code>: Original <code>value</code> column scaled between 0 and 1 using MinMaxScaler.</li> <li><code>anomaly_score</code>: Raw prediction from Isolation Forest (1 = normal, -1 = anomaly).</li> <li><code>anomaly</code>: Converted to binary indicator (0 = normal, 1 = anomaly).</li> </ul> </li> </ul>"},{"location":"02_preprocessing_and_baseline_iforest/#print-anomaly-summary-count","title":"Print anomaly summary count\u00b6","text":"<p>We print a summary of how many points have been classified as normal (<code>0</code>) and as anomalies (<code>1</code>). This provides a quick check on the proportion of detected anomalies in the dataset.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#anomaly-counts-summary","title":"Anomaly Counts Summary\u00b6","text":"<ul> <li><p>Out of 7501 total samples:</p> <ul> <li>7127 are predicted as normal (<code>anomaly = 0</code>).</li> <li>374 are flagged as anomalies (<code>anomaly = 1</code>).</li> </ul> </li> <li><p>This matches the initial contamination setting of 5% in the Isolation Forest model.</p> </li> <li><p>Educational Note: The anomaly detection successfully identified a small fraction of observations as anomalous, which we will later explore further using dimensionality reduction and clustering methods in Week 3.</p> </li> </ul>"},{"location":"02_preprocessing_and_baseline_iforest/#step-8-visualisation","title":"Step 8 - Visualisation\u00b6","text":"<p>Visualisation helps us interpret model outputs by placing them in the context of the original signal. By plotting the data, we can visually assess whether detected anomalies align with expected patterns or reveal unexpected behaviour.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#plot-original-signal","title":"Plot original signal\u00b6","text":"<p>We plot the original signal to provide a reference for further visualisation steps. This forms the baseline against which anomalies and other processed outputs will be compared.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#plot-description","title":"Plot Description:\u00b6","text":"<p>The plot displays the original signal across the full timestamp range. We observe a strong periodic pattern with consistent cycles and amplitude fluctuations between approximately 66 and 110. This visual baseline will help us interpret where anomalies may appear when overlaid in later steps.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#overlay-detected-anomalies","title":"Overlay detected anomalies\u00b6","text":"<p>We now visualise the anomalies detected by the Isolation Forest by overlaying them on the original signal. This helps us assess whether the flagged points align with expected irregularities in the data.</p> <p>The <code>plt.scatter()</code> function highlights points classified as anomalies (<code>anomaly == 1</code>) in red with 'x' markers. The plot includes labels, gridlines, and a legend for clarity.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#detected-anomalies-observations","title":"Detected Anomalies - Observations\u00b6","text":"<p>The time series plot above shows the full InternalBleeding14 dataset after applying the Isolation Forest model.</p> <ul> <li>The blue curve represents the complete signal across all timestamps.</li> <li>The red 'x' markers indicate data points flagged as anomalies by the model.</li> </ul> <p>This visualisation allows us to verify that the Isolation Forest correctly identifies both unusually high peaks and abnormally low troughs in the signal, which represent deviations from the expected cyclic pattern. The model successfully isolates outliers that deviate from the typical oscillating behaviour of the physiological sensor data.</p> <p>This plot serves as a quick initial verification that the model is operational and sensitive to unusual fluctuations. Detailed validation will follow in later stages.</p>"},{"location":"02_preprocessing_and_baseline_iforest/#extension-exercise-apply-isolation-forest-to-the-fantasia-dataset","title":"Extension Exercise: Apply Isolation Forest to the Fantasia Dataset\u00b6","text":"<p>Now that you have verified the Isolation Forest model on the <code>InternalBleeding14</code> dataset, you are invited to apply the same anomaly detection pipeline to a second time series: <code>209Fantasia_clean.csv</code>.</p> <p>This dataset, derived from the UCR Time Series Anomaly Archive (2021), contains 92,000 continuous measurements that simulate periodic signal fluctuations. Unlike the physiological spikes in InternalBleeding14, Fantasia presents a cleaner, more stable waveform; ideal for modelling in contexts such as healthcare, transport, or sensor monitoring.</p> <p>You are encouraged to:</p> <ul> <li>Load the Fantasia dataset from the <code>/data</code> folder</li> <li>Apply the same preprocessing and Isolation Forest model</li> <li>Plot the detected anomalies using the familiar red 'x' visualisation</li> <li>Compare the anomaly patterns against what you observed in the previous dataset</li> </ul> <p>Reflection prompts:</p> <ul> <li>How do the results differ in scale and pattern?</li> <li>Are anomalies easier or harder to interpret in Fantasia?</li> <li>Would you adjust Isolation Forest parameters in light of the signal quality?</li> </ul> <p>This exercise reinforces your understanding of how anomaly detection behaves across datasets with differing rhythm, noise levels, and domain characteristics.</p>"},{"location":"03_dimensionality_and_clustering/","title":"03 Dimensionality and Clustering","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nimport hdbscan\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.preprocessing import MinMaxScaler from sklearn.ensemble import IsolationForest from sklearn.decomposition import PCA import hdbscan import warnings warnings.filterwarnings(\"ignore\", category=FutureWarning)  In\u00a0[\u00a0]: Copied! <pre>file_path = 'data/InternalBleeding14.csv'\ndf = pd.read_csv(file_path, sep=\",\")\n\nprint(\"\\nFirst 10 rows of dataset:\")\ndisplay(df.head(10))\n</pre> file_path = 'data/InternalBleeding14.csv' df = pd.read_csv(file_path, sep=\",\")  print(\"\\nFirst 10 rows of dataset:\") display(df.head(10)) <pre>\nFirst 10 rows of dataset:\n</pre> timestamp value 0 0 97.46170 1 1 97.38159 2 2 97.18323 3 3 96.96197 4 4 96.67206 5 5 96.55380 6 6 96.62247 7 7 96.48132 8 8 96.52329 9 9 96.79413 In\u00a0[\u00a0]: Copied! <pre>scaler = MinMaxScaler()\ndf['value_scaled'] = scaler.fit_transform(df[['value']])\n\niso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\ndf['anomaly_score'] = iso.fit_predict(df[['value_scaled']])\ndf['anomaly'] = (df['anomaly_score'] == -1).astype(int)\n\nprint(\"\\nSample of dataset with anomaly predictions:\")\ndisplay(df.head(10))\n\nprint(\"\\nAnomaly counts:\")\ndisplay(df['anomaly'].value_counts())\n</pre> scaler = MinMaxScaler() df['value_scaled'] = scaler.fit_transform(df[['value']])  iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42) df['anomaly_score'] = iso.fit_predict(df[['value_scaled']]) df['anomaly'] = (df['anomaly_score'] == -1).astype(int)  print(\"\\nSample of dataset with anomaly predictions:\") display(df.head(10))  print(\"\\nAnomaly counts:\") display(df['anomaly'].value_counts()) <pre>\nSample of dataset with anomaly predictions:\n</pre> timestamp value value_scaled anomaly_score anomaly 0 0 97.46170 0.733364 1 0 1 1 97.38159 0.731460 1 0 2 2 97.18323 0.726746 1 0 3 3 96.96197 0.721487 1 0 4 4 96.67206 0.714597 1 0 5 5 96.55380 0.711786 1 0 6 6 96.62247 0.713418 1 0 7 7 96.48132 0.710064 1 0 8 8 96.52329 0.711061 1 0 9 9 96.79413 0.717498 1 0 <pre>\nAnomaly counts:\n</pre> count anomaly 0 7127 1 374 dtype: int64 In\u00a0[\u00a0]: Copied! <pre>pca = PCA(n_components=1)\ndf['pca_component'] = pca.fit_transform(df[['value_scaled']])\n\nprint(\"\\nSample of PCA-transformed data:\")\ndisplay(df.head(10))\n</pre> pca = PCA(n_components=1) df['pca_component'] = pca.fit_transform(df[['value_scaled']])  print(\"\\nSample of PCA-transformed data:\") display(df.head(10)) <pre>\nSample of PCA-transformed data:\n</pre> timestamp value value_scaled pca_component cluster anomaly_score anomaly 0 0 97.46170 0.733364 0.300088 1 1 0 1 1 97.38159 0.731460 0.298184 1 1 0 2 2 97.18323 0.726746 0.293470 1 1 0 3 3 96.96197 0.721487 0.288211 1 1 0 4 4 96.67206 0.714597 0.281321 1 1 0 5 5 96.55380 0.711786 0.278510 1 1 0 6 6 96.62247 0.713418 0.280142 1 1 0 7 7 96.48132 0.710064 0.276788 1 1 0 8 8 96.52329 0.711061 0.277785 1 1 0 9 9 96.79413 0.717498 0.284222 1 1 0 In\u00a0[\u00a0]: Copied! <pre>clusterer = hdbscan.HDBSCAN(min_cluster_size=50)\ndf['cluster'] = clusterer.fit_predict(df[['pca_component']])\n\nprint(\"\\nSample of data with cluster assignments:\")\ndisplay(df.head(10))\n\nprint(\"\\nCluster counts:\")\ndisplay(df['cluster'].value_counts())\n</pre> clusterer = hdbscan.HDBSCAN(min_cluster_size=50) df['cluster'] = clusterer.fit_predict(df[['pca_component']])  print(\"\\nSample of data with cluster assignments:\") display(df.head(10))  print(\"\\nCluster counts:\") display(df['cluster'].value_counts())  <pre>\nSample of data with cluster assignments:\n</pre> timestamp value value_scaled pca_component cluster anomaly_score anomaly 0 0 97.46170 0.733364 0.300088 1 1 0 1 1 97.38159 0.731460 0.298184 1 1 0 2 2 97.18323 0.726746 0.293470 1 1 0 3 3 96.96197 0.721487 0.288211 1 1 0 4 4 96.67206 0.714597 0.281321 1 1 0 5 5 96.55380 0.711786 0.278510 1 1 0 6 6 96.62247 0.713418 0.280142 1 1 0 7 7 96.48132 0.710064 0.276788 1 1 0 8 8 96.52329 0.711061 0.277785 1 1 0 9 9 96.79413 0.717498 0.284222 1 1 0 <pre>\nCluster counts:\n</pre> count cluster 1 7173 -1 229 0 99 dtype: int64 In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16, 8))\ncolors = {0: 'orange', 1: 'blue', 2: 'green', -1: 'red'}\nfor cluster, color in colors.items():\n    subset = df[df['cluster'] == cluster]\n    plt.scatter(subset['timestamp'], subset['pca_component'],\n                c=color, label=f'Cluster {cluster}', s=10)\nplt.title('HDBSCAN Cluster Assignments on PCA Component')\nplt.xlabel('Timestamp')\nplt.ylabel('PCA Component')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> plt.figure(figsize=(16, 8)) colors = {0: 'orange', 1: 'blue', 2: 'green', -1: 'red'} for cluster, color in colors.items():     subset = df[df['cluster'] == cluster]     plt.scatter(subset['timestamp'], subset['pca_component'],                 c=color, label=f'Cluster {cluster}', s=10) plt.title('HDBSCAN Cluster Assignments on PCA Component') plt.xlabel('Timestamp') plt.ylabel('PCA Component') plt.legend() plt.grid() plt.show()  In\u00a0[\u00a0]: Copied! <pre>file_path = 'data/209Fantasia_clean.csv'\n</pre> file_path = 'data/209Fantasia_clean.csv'"},{"location":"03_dimensionality_and_clustering/#03-dimensionality-and-clustering","title":"03 Dimensionality and Clustering\u00b6","text":"<p>This notebook begins the Week 3 task, building on the work completed in Weeks 1 and 2. We first repeat key preprocessing steps, apply Isolation Forest for baseline anomaly detection, and then extend the analysis using Principal Component Analysis (PCA) and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN).</p> <p>PCA is a dimensionality reduction technique that transforms the data into a lower-dimensional space, helping to highlight patterns and structure while retaining key variance. HDBSCAN is an advanced clustering method that identifies groups of similar points and isolates noise without needing to predefine the number of clusters.</p> <p>Together, these techniques allow us to explore latent structure in the data and detect subtle anomalies beyond what basic isolation methods can reveal.</p>"},{"location":"03_dimensionality_and_clustering/#step-1-import-required-libraries","title":"Step 1 - Import Required Libraries\u00b6","text":"<p>We begin by importing the necessary libraries for data handling, visualisation, scaling, and anomaly detection, as introduced in Weeks 1 and 2. These provide the core tools for preparing the data, plotting results, applying scaling, and running Isolation Forest as our baseline model.</p> <p>For Week 3, we additionally import:</p> <ul> <li><code>PCA</code> (from <code>sklearn.decomposition</code>) - used to reduce the dimensionality of the dataset, allowing us to uncover latent patterns and simplify the data structure while retaining key variance.</li> <li><code>hdbscan</code>- a powerful clustering algorithm that groups similar data points and identifies noise without needing a predefined number of clusters, helping us detect more complex structures and subtle anomalies.</li> </ul> <p>These additions extend our toolkit to move beyond basic anomaly detection and explore the underlying structure of the data.</p>"},{"location":"03_dimensionality_and_clustering/#step-2-codes-rrecap","title":"Step 2  - Codes Rrecap\u00b6","text":"<p>As covered in previous weeks, we repeat the following steps: defining the file path, loading the dataset, normalising the value column using Min-Max scaling, applying Isolation Forest for anomaly detection, converting the anomaly output, and visualising detected anomalies by overlaying them on the original signal.</p> <p>For full code explanations, please refer to Week 2.</p>"},{"location":"03_dimensionality_and_clustering/#step-3-apply-pca-reduce-to-1-component-for-clustering","title":"Step 3  - Apply PCA (Reduce to 1 Component for Clustering)\u00b6","text":"<p>We apply Principal Component Analysis (PCA) to reduce the scaled signal to a single component. This simplifies the data while preserving key variance, making it suitable for clustering.</p>"},{"location":"03_dimensionality_and_clustering/#observations-from-sample-of-pca-transformed-dataset","title":"Observations from Sample of PCA-transformed dataset:\u00b6","text":"<ul> <li><code>timestamp</code>: Index position of each observation.</li> <li><code>value</code>: Original sensor reading.</li> <li><code>value_scaled</code>: Normalised version of <code>value</code> between 0 and 1.</li> <li><code>pca_component</code>: Result of applying Principal Component Analysis (PCA) to reduce dimensionality; represents the data in a compressed feature space while preserving variance.</li> <li><code>cluster</code>: Cluster label assigned by HDBSCAN (retained for full pipeline consistency).</li> <li><code>anomaly_score</code>: Raw prediction from Isolation Forest.</li> <li><code>anomaly</code>: Final binary label where <code>0</code> means normal, <code>1</code> indicates anomaly.</li> </ul> <p>PCA simplifies the data structure for easier clustering by converting multiple correlated features into an uncorrelated principal component while retaining essential variance patterns in the signal.</p>"},{"location":"03_dimensionality_and_clustering/#step-4-apply-hdbscan","title":"Step 4 - Apply HDBSCAN\u00b6","text":"<p>We apply HDBSCAN clustering to the PCA-transformed data. HDBSCAN identifies clusters of varying density and shape, and automatically labels noise points, allowing us to detect natural groupings without predefining the number of clusters. We then display a sample of the cluster assignments and show the count of points in each cluster for verification.</p>"},{"location":"03_dimensionality_and_clustering/#observations-from-hdbscan-cluster-assignments-and-cluster-distribution","title":"Observations from HDBSCAN Cluster Assignments and Cluster distribution\u00b6","text":"<ul> <li><code>timestamp</code>: Position index for the time series.</li> <li><code>value</code>: Original observed sensor value.</li> <li><code>value_scaled</code>: Normalised sensor value (scaled between 0 and 1).</li> <li><code>pca_component</code>: Principal component output after dimensionality reduction.</li> <li><code>cluster</code>: Assigned cluster label generated by HDBSCAN.<ul> <li><code>1</code>, <code>0</code>: Represent stable clusters.</li> <li><code>-1</code>: Represents noise points or data that HDBSCAN could not confidently assign to any cluster.</li> </ul> </li> <li><code>anomaly_score</code> &amp; <code>anomaly</code>: Carried forward from previous Isolation Forest step for consistency.-</li> </ul>"},{"location":"03_dimensionality_and_clustering/#cluster-distribution","title":"Cluster distribution\u00b6","text":"<ul> <li>Cluster <code>1</code>: 7173 samples (main normal structure).</li> <li>Cluster <code>0</code>: 99 samples (possible alternative minor behaviour).</li> <li>Cluster <code>-1</code>: 229 noise points (potential irregular behaviour or weak structure).</li> </ul> <p>Clustering allows grouping of similar behaviour patterns and provides an unsupervised structure to support anomaly interpretation and model validation.</p>"},{"location":"03_dimensionality_and_clustering/#step-5-visualise-cluster-assignments","title":"Step 5 - Visualise Cluster Assignments\u00b6","text":"<p>We plot the PCA component over time and colour each point by its HDBSCAN cluster assignment. This allows us to see how the identified clusters and noise points are distributed across the time series.</p>"},{"location":"03_dimensionality_and_clustering/#observations-from-hdbscan-cluster-assignments-pca-projection-plot","title":"Observations from HDBSCAN Cluster Assignments - PCA Projection Plot\u00b6","text":"<p>This scatter plot visualises the HDBSCAN cluster assignments after dimensionality reduction with PCA.</p> <ul> <li>X-axis (Timestamp): The sequence index of the time series data.</li> <li>Y-axis (PCA Component): The reduced representation of the normalised signal using Principal Component Analysis (PCA).</li> <li>Coloured markers:<ul> <li>Cluster 1 (blue): The dominant stable pattern across most of the time series.</li> <li>Cluster 0 (orange): A smaller sub-pattern possibly reflecting moderate variation.</li> <li>Cluster 2 (green): Not present in this dataset \u2014 plotted for completeness.</li> <li>Noise cluster -1 (red): Points which could not be assigned to any stable cluster, likely representing irregular behaviour, minor local deviations, or weak structures.</li> </ul> </li> </ul> <p>Interpretation:</p> <ul> <li>The majority of points are assigned to Cluster 1, indicating highly consistent normal behaviour.</li> <li>The noise points (<code>-1</code>) suggest areas of the time series where data deviates from dominant structures but not necessarily strong anomalies. The projection confirms that after PCA transformation, HDBSCAN successfully separates stable signal dynamics from minor irregularities.</li> </ul>"},{"location":"03_dimensionality_and_clustering/#extension-exercise-clustering-anomaly-windows-in-the-fantasia-dataset","title":"Extension Exercise: Clustering Anomaly Windows in the Fantasia Dataset\u00b6","text":"<p>To deepen your understanding of dimensionality reduction and clustering, apply the same pipeline (windowing \u2192 PCA \u2192 HDBSCAN) to the <code>209Fantasia_clean.csv</code> dataset, found in the <code>/data</code> folder.</p> <p>Fantasia presents a stable, nearly sinusoidal waveform with subtle deviations; making it an excellent contrast to the spiky, irregular InternalBleeding14 signal. It is particularly well suited for detecting anomaly windows that reflect smooth behavioural drift or rhythmic disruption.</p> <p>Your task:</p> <ul> <li>Load the Fantasia dataset and apply the same sliding window embedding</li> <li>Standardise and reduce the dimensionality with PCA</li> <li>Fit HDBSCAN to discover natural groupings (clusters) and outliers</li> <li>Visualise the results using UMAP or PCA scatter plots, and optionally the original time series</li> </ul> <p>Reflection prompts:</p> <ul> <li>Do clusters in Fantasia appear more distinct or diffuse compared to InternalBleeding14?</li> <li>Are outliers more subtle, and how does HDBSCAN respond?</li> <li>How might you tune the window size or PCA components to better capture rhythmic deviation?</li> </ul> <p>This exercise invites you to explore anomaly detection in a smoother, lower-noise context, refining your sense of how signal properties affect cluster structure.</p>"},{"location":"04_model_interpretation_and_explanation/","title":"04 Model Interpretation &amp; Explanation","text":""},{"location":"04_model_interpretation_and_explanation/#04-model-interpretation-explanation","title":"04 Model Interpretation &amp; Explanation\u00b6","text":"<p>This notebook in Week 4 focuses on interpreting the results of our anomaly detection pipeline using Isolation Forest and HDBSCAN clustering. Building on the outputs of Notebooks 01-03, we draw behavioural insights and visual inferences from the models, without introducing new code logic. The focus here lies in understanding what the models reveal about the signal: its structure, its irregularities, and its interpretive value for real-world applications.</p>"},{"location":"04_model_interpretation_and_explanation/#step-1-summary-of-the-pipeline","title":"Step 1 - Summary of the Pipeline\u00b6","text":"<p>The ReCoDE exemplar pipeline follows a modular structure:</p> <ul> <li>Data Preparation: Load and verify the InternalBleeding14 time series dataset.</li> <li>Preprocessing: Apply MinMaxScaler to normalise signal values between 0 and 1.</li> <li>Isolation Forest: Detect anomalies using unsupervised tree-based scoring (5% contamination).</li> <li>PCA Transformation: Reduce data dimensionality to a single component.</li> <li>HDBSCAN Clustering: Discover latent behavioural clusters and noise points.</li> <li>Visualisation: Overlay plots to reveal anomaly distribution and cluster structures.</li> </ul>"},{"location":"04_model_interpretation_and_explanation/#step-2-model-assumptions","title":"Step 2 - Model Assumptions\u00b6","text":"<ul> <li>The time series displays periodic behaviour with sporadic anomalies.</li> <li>Anomalies are rare and sparsely distributed.</li> <li>PCA captures meaningful signal variance in a single component.</li> <li>HDBSCAN does not require a pre-set number of clusters.</li> <li>Cluster <code>-1</code> (noise) may indicate either outliers or weak patterning.</li> </ul>"},{"location":"04_model_interpretation_and_explanation/#step-3-isolation-forest-results","title":"Step 3 - Isolation Forest Results\u00b6","text":"<p>The Isolation Forest model detected anomalies by evaluating how easily each data point could be isolated in the scaled value space. This method identified 374 anomalous points, out of a total of 7501 observations; approximately 5%, consistent with the model\u2019s contamination parameter. These points typically reflect abrupt deviations from regular amplitude oscillations.</p>"},{"location":"04_model_interpretation_and_explanation/#step-4-hdbscan-clustering-insights","title":"Step 4 - HDBSCAN Clustering Insights\u00b6","text":"<p>HDBSCAN grouped data points into behavioural clusters using the PCA-reduced representation. The model revealed:</p> <ul> <li><p>Cluster 1; 7173 points (dominant behavioural group)</p> </li> <li><p>Cluster 0; 99 points (minor variation group)</p> </li> <li><p>Noise cluster (-1); 229 points flagged as unclassifiable</p> </li> </ul> <p>This clustering structure reinforces the presence of two distinct behavioural regimes; noise points may overlap with regions identified as anomalous earlier. The model offers an interpretable lens for understanding dense versus irregular signal patterns.</p>"},{"location":"04_model_interpretation_and_explanation/#step-5-commentary-on-cross-validation-in-unsupervised-pipelines","title":"Step 5 - Commentary on Cross-Validation in Unsupervised Pipelines\u00b6","text":"<p>In supervised learning, cross-validation typically relies on labelled data to assess model performance. However, in unsupervised anomaly detection, where labels are unavailable, such validation becomes more interpretive than definitive. In this step, we focus instead on the robustness, overlap, and divergence of outputs generated by different models in the pipeline.</p> <p>This exemplar makes use of two complementary unsupervised methods:</p> <ul> <li>Isolation Forest (IF) identifies anomalies by recursively partitioning the data and isolating points that require fewer splits.</li> <li>HDBSCAN labels points as noise or assigns them to clusters based on local density and hierarchical structure.</li> </ul> <p>Rather than treating either model as authoritative, we encourage learners to reflect on their interaction and internal consistency.</p>"},{"location":"04_model_interpretation_and_explanation/#suggested-exploratory-checks","title":"Suggested Exploratory Checks:\u00b6","text":"<ul> <li><p>Overlap: Compare the points flagged as anomalies by Isolation Forest with those labelled as noise by HDBSCAN. High overlap may suggest shared detection of structural irregularity.</p> </li> <li><p>Divergence: Examine whether any Isolation Forest outliers are embedded within HDBSCAN clusters. These may reflect globally anomalous but locally typical behaviour, which is especially relevant in behavioural or transport modelling.</p> </li> <li><p>Stability: Run the pipeline on multiple random subsets of the data. Assess the consistency of model outputs using:</p> <ul> <li>Internal validation metrics such as the Silhouette Score or Davies\u2013Bouldin Index.</li> <li>Set-based measures such as Jaccard Similarity to quantify overlap in anomaly labels across subsets.</li> </ul> </li> </ul> <p>These steps help learners form a critical view of unsupervised outputs without relying on ground truth, promoting deeper reasoning about the model\u2019s behaviour and limitations.</p>"},{"location":"04_model_interpretation_and_explanation/#optional-extension","title":"Optional Extension:\u00b6","text":"<p>Learners may explore a basic form of pseudo cross-validation by applying the full pipeline to repeated subsamples. Anomaly scores, cluster assignments, and overlap metrics may be tracked across runs to assess the reliability of results. This remains an optional activity and is intended to build confidence rather than serve as formal validation.</p>"},{"location":"04_model_interpretation_and_explanation/#step-6-visual-patterns-and-significance","title":"Step 6 - Visual Patterns and Significance\u00b6","text":"<p>Interpreting the structure and meaning of anomalies</p> <p>After detecting anomalies and clusters, it is essential to examine their visual context and practical significance. Anomalies should not be treated as definitive errors or artefacts. Rather, they require interpretation within the structure of the time series or the reduced latent space.</p> <p>In this step, learners are encouraged to:</p> <ul> <li><p>Plot the original signal with detected anomaly points overlaid. Reflect on their position. Do they appear at signal boundaries, isolated peaks, or periods of sudden fluctuation?</p> </li> <li><p>Review dimensionality-reduced embeddings, such as UMAP or PCA projections, with cluster labels and anomalies highlighted. Consider whether anomalies fall outside dense regions, or whether they cluster in unexpected areas.</p> </li> <li><p>Annotate the behavioural or operational context, for example:</p> <ul> <li>\u201cAnomalies are concentrated at signal troughs. This may indicate sensor dropout.\u201d</li> <li>\u201cOutliers emerge during transitions between regimes. These may represent decision delays or hesitation.\u201d</li> <li>\u201cHigh anomaly scores occur near maximum amplitude. This is often characteristic of sensor saturation.\u201d</li> </ul> </li> </ul> <p>The objective here is to encourage a form of practical reasoning about model outputs. Not all anomalies are mistakes. Some may represent rare but valid occurrences, while others may arise from known artefacts such as sensor noise, boundary distortion, or missing data.</p> <p>This interpretive practice supports learners in moving from algorithmic output to meaningful, defensible insights.</p>"},{"location":"04_model_interpretation_and_explanation/#step-7-educational-commentary","title":"Step 7 - Educational Commentary\u00b6","text":"<p>This notebook demonstrates the value of combining multiple unsupervised techniques for time series interpretation. Isolation Forest and HDBSCAN offer complementary strengths:</p> <ul> <li>Tree-based anomaly scoring (Isolation Forest)</li> <li>Density-based segmentation (HDBSCAN)</li> </ul> <p>Together, they provide a layered understanding of behavioural structure within unlabelled, physiologically-inspired data. This approach generalises well to transport analysis, medical monitoring, and public service data pipelines where labels are scarce but signal integrity is vital.</p>"},{"location":"04_model_interpretation_and_explanation/#reflective-task","title":"Reflective Task\u00b6","text":"<p>Learners may now revisit anomaly visualisation in Notebook 03 and reflect on the behavioural meaning of detected points. Are any patterns consistent with artefacts? Which anomalies appear explainable versus unexpected?</p>"},{"location":"04_model_interpretation_and_explanation/#further-reading","title":"Further Reading\u00b6","text":"<ul> <li>Rousseeuw, P.J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, pp. 53\u201365.</li> <li>Chandola, V., Banerjee, A. and Kumar, V. (2009). Anomaly detection: A survey. ACM Computing Surveys, 41(3), Article 15.</li> <li>Aggarwal, C.C. (2017). Outlier Analysis. 2nd edn. Springer.</li> <li>Campello, R.J.G.B., Moulavi, D. and Sander, J. (2013). Density-based clustering based on hierarchical density estimates. PAKDD 2013, pp. 160\u2013172.</li> <li>Efron, B. and Tibshirani, R.J. (1994). An Introduction to the Bootstrap. CRC Press.</li> </ul>"},{"location":"05_ethics_and_limitations/","title":"05 Ethical Reflection and False Positive Risks","text":""},{"location":"05_ethics_and_limitations/#05-ethical-reflection-and-false-positive-risks","title":"05  Ethical Reflection and False Positive Risks\u00b6","text":"<p>This notebook explores key ethical concerns in unsupervised anomaly detection, particularly in behavioural or operational contexts. Learners are encouraged to reflect critically on the social impact, fairness, and responsibility of deploying such models in real-world systems.</p>"},{"location":"05_ethics_and_limitations/#step-1-ethical-risks-in-anomaly-detection","title":"Step 1 - Ethical Risks in Anomaly Detection\u00b6","text":"<p>Opacity in algorithmic reasoning remains a key concern, particularly in domains where decisions carry social or reputational consequences. Black-box models, such as deep neural networks, may yield high-performance outcomes but often lack transparency in how conclusions are drawn. In contexts like finance or behavioural monitoring, this can lead to automated decisions without clear justification.</p>"},{"location":"05_ethics_and_limitations/#step-2-data-bias-and-model-fairness","title":"Step 2 - Data Bias and Model Fairness\u00b6","text":"<p>As demonstrated in Ludera (2021), fraud detection datasets are often highly imbalanced, with fraudulent transactions forming only a small minority. Without appropriate preprocessing such as SMOTE-ENN or other class-balancing techniques, machine learning models tend to overfit to the dominant class and may wrongly learn to treat legitimate transactions as anomalous. These misclassifications arise not from genuine behavioural irregularity but from the model's distorted understanding of rarity. If left uncorrected, such hallucinations can result in systemic bias against honest users.</p>"},{"location":"05_ethics_and_limitations/#step-3-the-impact-of-false-positives","title":"Step 3 - The Impact of False Positives\u00b6","text":"<p>As shown in Fawei &amp; Ludera (2020), even in non-sensitive domains such as marketing, misclassification can carry significant consequences. In the direct marketing experiments, the model was trained on a dataset where the majority of customers declined the offer. Without careful calibration, the system risked overlooking genuinely interested customers, simply because their profiles differed from the dominant pattern. By tuning error cost parameters and interpreting the confusion matrix, the study demonstrated how important it is to minimise false rejections, even when ground truth is not uniformly distributed.</p> <p>Similar risks were later explored in Ludera (2021), where false positives in credit card fraud detection could lead to service blocks for legitimate users. This highlights the broader implications of imbalance-related misclassification across domains, particularly where the cost of incorrect labelling is socially or financially significant.</p>"},{"location":"05_ethics_and_limitations/#step-4-illustrative-scenario","title":"Step 4 - Illustrative Scenario\u00b6","text":"<p>Imagine a public service system where user behaviour, such as late attendance or sporadic access, is monitored for anomalies. A low-income user who works night shifts may present unusual usage patterns. If the model was trained predominantly on daytime users, this behaviour could be wrongly flagged, potentially limiting access to essential services or triggering further surveillance.</p>"},{"location":"05_ethics_and_limitations/#step-5-proposed-mitigations-and-safeguards","title":"Step 5 - Proposed Mitigations and Safeguards\u00b6","text":"<p>In line with established principles for trustworthy AI, several safeguards may be introduced to reduce the harm caused by automated misclassifications. These include embedding interpretability techniques, applying calibrated anomaly score thresholds, and implementing human fallback review layers.</p> <p>In high-risk domains, models should incorporate audit trails to ensure accountability, and all flagged outputs ought to be subject to proportional, human-led interpretation prior to any consequential action. These recommendations are consistent with international guidelines on AI ethics, which emphasise transparency, oversight, and the prioritisation of human well-being [3], [4].</p>"},{"location":"05_ethics_and_limitations/#step-6-learner-reflection-questions","title":"Step 6 - Learner Reflection Questions\u00b6","text":"<ul> <li>How should anomalies be handled when no ground truth is available?</li> <li>Should users be informed if their behaviour is being flagged by automated systems?</li> <li>How can developers mitigate harm from false positives while preserving model sensitivity?</li> </ul>"},{"location":"05_ethics_and_limitations/#references","title":"References\u00b6","text":"<ol> <li><p>Ludera, D.T.J. (2021). Credit Card Fraud Detection by Combining Synthetic Minority Oversampling and Edited Nearest Neighbours. In: FICC 2021 - Future of Information and Communication Conference.</p> </li> <li><p>Fawei, T. &amp; Ludera, D.T.J. (2020). Data Mining Solutions for Direct Marketing Campaigns. Intelligent Systems and Applications.</p> </li> <li><p>European Commission (High-Level Expert Group on Artificial Intelligence) (2019). Ethics Guidelines for Trustworthy Artificial Intelligence. Brussels: European Commission. Available at: https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai (Accessed: 10 July 2025).</p> </li> <li><p>IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (2017). Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (Version 2). New York: Institute of Electrical and Electronics Engineers. Available at: https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf (Accessed: 10 July 2025).</p> </li> </ol>"},{"location":"06_visual_polishing_and_citations/","title":"06 Visual Refinement and Method Citations","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nimport hdbscan\n\nfile_path = 'data/InternalBleeding14.csv'\ndf = pd.read_csv(file_path, sep=\",\")\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.ensemble import IsolationForest from sklearn.decomposition import PCA import hdbscan  file_path = 'data/InternalBleeding14.csv' df = pd.read_csv(file_path, sep=\",\") In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16, 5.5))\n\nplt.plot(df['timestamp'], df['value'], color='#3366cc', linewidth=1.5, label='Signal')\n\nplt.title(\"Original Signal\", fontsize=15, weight='bold')\nplt.xlabel(\"Time\", fontsize=12)\nplt.ylabel(\"Signal Value\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\n\ncurrent_max_time = df['timestamp'].max()\nplt.xlim(-400, current_max_time + 800)\nplt.legend(loc='upper right', fontsize=11, frameon=True)\n\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(16, 5.5))  plt.plot(df['timestamp'], df['value'], color='#3366cc', linewidth=1.5, label='Signal')  plt.title(\"Original Signal\", fontsize=15, weight='bold') plt.xlabel(\"Time\", fontsize=12) plt.ylabel(\"Signal Value\", fontsize=12) plt.grid(True, linestyle='--', alpha=0.5)  current_max_time = df['timestamp'].max() plt.xlim(-400, current_max_time + 800) plt.legend(loc='upper right', fontsize=11, frameon=True)  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n\ndf['anomaly_score'] = isolation_forest.fit_predict(df[['value']])\ndf['anomaly'] = df['anomaly_score'] == -1\n\nplt.figure(figsize=(16, 5.5))\n\nplt.plot(df['timestamp'], df['value'], color='#3366cc', linewidth=1.5, label='Signal')\n\nplt.scatter(df[df['anomaly']]['timestamp'], df[df['anomaly']]['value'],\n            color='red', marker='x', s=40, label='Anomaly', zorder=3)\n\nplt.title(\"Anomaly Detection using Isolation Forest\", fontsize=15, weight='bold')\nplt.xlabel(\"Time\", fontsize=12)\nplt.ylabel(\"Signal Value\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\n\ncurrent_max_time = df['timestamp'].max()\nplt.xlim(-400, current_max_time + 800)\nplt.legend(loc='upper right', fontsize=11, frameon=True)\n\nplt.tight_layout()\nplt.show()\n</pre> isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)  df['anomaly_score'] = isolation_forest.fit_predict(df[['value']]) df['anomaly'] = df['anomaly_score'] == -1  plt.figure(figsize=(16, 5.5))  plt.plot(df['timestamp'], df['value'], color='#3366cc', linewidth=1.5, label='Signal')  plt.scatter(df[df['anomaly']]['timestamp'], df[df['anomaly']]['value'],             color='red', marker='x', s=40, label='Anomaly', zorder=3)  plt.title(\"Anomaly Detection using Isolation Forest\", fontsize=15, weight='bold') plt.xlabel(\"Time\", fontsize=12) plt.ylabel(\"Signal Value\", fontsize=12) plt.grid(True, linestyle='--', alpha=0.5)  current_max_time = df['timestamp'].max() plt.xlim(-400, current_max_time + 800) plt.legend(loc='upper right', fontsize=11, frameon=True)  plt.tight_layout() plt.show()  In\u00a0[\u00a0]: Copied! <pre>scaler = MinMaxScaler()\ndf['value_scaled'] = scaler.fit_transform(df[['value']])\n\npca = PCA(n_components=1)\ndf['pca_component'] = pca.fit_transform(df[['value_scaled']])\n\niso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\ndf['anomaly_score'] = iso.fit_predict(df[['value_scaled']])\ndf['anomaly'] = (df['anomaly_score'] == -1).astype(int)\n\nclusterer = hdbscan.HDBSCAN(min_cluster_size=50)\ndf['cluster'] = clusterer.fit_predict(df[['pca_component']])\n</pre> scaler = MinMaxScaler() df['value_scaled'] = scaler.fit_transform(df[['value']])  pca = PCA(n_components=1) df['pca_component'] = pca.fit_transform(df[['value_scaled']])  iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42) df['anomaly_score'] = iso.fit_predict(df[['value_scaled']]) df['anomaly'] = (df['anomaly_score'] == -1).astype(int)  clusterer = hdbscan.HDBSCAN(min_cluster_size=50) df['cluster'] = clusterer.fit_predict(df[['pca_component']]) In\u00a0[\u00a0]: Copied! <pre>iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\ndf['anomaly_score'] = iso.fit_predict(df[['value_scaled']])\ndf['anomaly'] = (df['anomaly_score'] == -1).astype(int)\n\nclusterer = hdbscan.HDBSCAN(min_cluster_size=50)\ndf['cluster'] = clusterer.fit_predict(df[['pca_component']])\n\n# Plot (with reduced density on blue)\nplt.figure(figsize=(16, 5.5))\ncolors = {0: 'orange', 1: 'blue', 2: 'green', -1: 'red'}\nlabels = {0: 'Cluster 0', 1: 'Cluster 1', 2: 'Cluster 2', -1: 'Noise'}\n\nfor cluster, color in colors.items():\n    subset = df[df['cluster'] == cluster]\n    marker_size = 8 if cluster == 1 else 14\n    plt.scatter(subset['timestamp'], subset['pca_component'],\n                c=color, s=marker_size, label=labels[cluster])\n\nplt.title(\"HDBSCAN Cluster Assignments on PCA Component\", fontsize=15, weight='bold')\nplt.xlabel(\"Timestamp\", fontsize=12)\nplt.ylabel(\"PCA Component\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\n\ncurrent_max_time = df['timestamp'].max()\nplt.xlim(-400, current_max_time + 800)\nplt.legend(loc='upper right', fontsize=11, frameon=True)\nplt.tight_layout()\nplt.show()\n</pre> iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42) df['anomaly_score'] = iso.fit_predict(df[['value_scaled']]) df['anomaly'] = (df['anomaly_score'] == -1).astype(int)  clusterer = hdbscan.HDBSCAN(min_cluster_size=50) df['cluster'] = clusterer.fit_predict(df[['pca_component']])  # Plot (with reduced density on blue) plt.figure(figsize=(16, 5.5)) colors = {0: 'orange', 1: 'blue', 2: 'green', -1: 'red'} labels = {0: 'Cluster 0', 1: 'Cluster 1', 2: 'Cluster 2', -1: 'Noise'}  for cluster, color in colors.items():     subset = df[df['cluster'] == cluster]     marker_size = 8 if cluster == 1 else 14     plt.scatter(subset['timestamp'], subset['pca_component'],                 c=color, s=marker_size, label=labels[cluster])  plt.title(\"HDBSCAN Cluster Assignments on PCA Component\", fontsize=15, weight='bold') plt.xlabel(\"Timestamp\", fontsize=12) plt.ylabel(\"PCA Component\", fontsize=12) plt.grid(True, linestyle='--', alpha=0.5)  current_max_time = df['timestamp'].max() plt.xlim(-400, current_max_time + 800) plt.legend(loc='upper right', fontsize=11, frameon=True) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>file_path = 'data/209Fantasia_clean.csv'\n</pre> file_path = 'data/209Fantasia_clean.csv'"},{"location":"06_visual_polishing_and_citations/#06-visual-refinement-and-method-citations","title":"06 Visual Refinement and Method Citations\u00b6","text":"<p>This notebook applies final presentation polish to key visualisations and documents method references used throughout the pipeline. The aim is to improve clarity, reproducibility, and scholarly completeness.</p>"},{"location":"06_visual_polishing_and_citations/#notebook-setup","title":"Notebook Setup\u00b6","text":"<p>We begin by importing the core libraries already used throughout the exemplar. These support plotting, anomaly scoring, and clustering. The cleaned dataset is reloaded to ensure consistency across the final visual outputs.</p>"},{"location":"06_visual_polishing_and_citations/#step-1-refined-full-time-series-plot","title":"Step 1 - Refined Full Time Series Plot\u00b6","text":"<p>We begin by re-plotting the full univariate signal using an improved visual format. This version maintains the established colour palette for continuity, while increasing line weight and applying a balanced figure size to improve clarity. A newly introduced legend supports visual decoding by clearly labelling the signal. Additionally, the x-axis has been extended slightly to provide better spacing around edge points and maintain visual symmetry.</p>"},{"location":"06_visual_polishing_and_citations/#observations-original-signal-overview","title":"Observations: Original Signal Overview\u00b6","text":"<p>This refined time series plot offers a clearer view of the full signal distribution across all timestamps. The enhanced line weight and extended horizontal axis reveal periodic structure and edge behaviour more distinctly than in earlier notebooks. The added legend confirms signal identity and brings the plot into alignment with standard visualisation practices used in data analysis.</p> <p>This view establishes the baseline against which subsequent anomaly overlays and clustering patterns can be compared.</p>"},{"location":"06_visual_polishing_and_citations/#step-2-anomaly-detection-using-isolation-forest","title":"Step 2 - Anomaly Detection using Isolation Forest\u00b6","text":"<p>We now generate the full anomaly overlay using the Isolation Forest model. This step maintains the same detection parameters and visual styling introduced earlier, but incorporates modest refinements to the layout and axis configuration to enhance readability in professional contexts.</p>"},{"location":"06_visual_polishing_and_citations/#observations-and-visual-improvements","title":"Observations and Visual Improvements\u00b6","text":"<p>This anomaly overlay uses the same model parameters and colour scheme as in the previous notebooks, but applies targeted refinements to improve presentation quality:</p> <ul> <li>Legend Placement: The legend is repositioned to the top-right margin and formatted cleanly, avoiding overlap with anomaly points.</li> <li>Plot Sizing: A wider canvas and extended x-axis range improve balance and prevent crowding at the plot boundaries.</li> <li>Typography: Title and axis fonts are adjusted for improved readability in teaching or publication contexts.</li> </ul> <p>These small but focused adjustments help formalise the visual output for professional settings without altering the underlying results.</p>"},{"location":"06_visual_polishing_and_citations/#maintaining-preprocessing-consistency","title":"Maintaining Preprocessing Consistency\u00b6","text":"<p>To ensure full alignment with previous notebooks, we retain the same preprocessing logic used in the anomaly detection and clustering pipeline:</p> <ul> <li>MinMax Scaling is reapplied to normalise the signal (<code>value_scaled</code>), ensuring compatibility with both PCA and clustering methods.</li> <li>PCA Transformation is then applied to the scaled signal, projecting it into a one-dimensional principal component (<code>pca_component</code>). This simplified representation captures the main structural variance and supports clearer segmentation.</li> <li>Isolation Forest is reused with the same parameters (<code>n_estimators=100</code>, <code>contamination=0.05</code>) to detect anomalous behaviour based on the scaled values.</li> <li>HDBSCAN Clustering is reapplied to the PCA output, using the same minimum cluster size (<code>min_cluster_size=50</code>) to preserve cluster consistency.</li> </ul> <p>These components are not modified further, as their role is to preserve the structure and reliability of the earlier pipeline while preparing for the improved visual outputs that follow. Their outputs are not re-printed here, as they have already been explained and verified in detail in the previous notebooks.</p>"},{"location":"06_visual_polishing_and_citations/#step-3-visual-refinement-of-cluster-assignments-over-time","title":"Step 3 - Visual Refinement of Cluster Assignments Over Time\u00b6","text":"<p>We revisit the HDBSCAN cluster assignments by plotting the first principal component (<code>pca_component</code>) across time. Each point is colour-coded by cluster, allowing us to see how distinct behavioural segments and noise points evolve temporally.</p> <p>This visualisation was introduced in the previous notebook. Here, we enhance its presentation with a larger canvas, consistent typography, extended x-axis bounds, and a clean top-right legend placement. These refinements improve visual alignment with the anomaly overlay and prepare the output for teaching or publication.</p>"},{"location":"06_visual_polishing_and_citations/#observations-and-visual-improvements","title":"Observations and Visual Improvements\u00b6","text":"<p>This updated cluster visualisation applies the same structural format and stylistic refinements introduced in earlier steps, ensuring consistency with the rest of the exemplar.</p> <ul> <li>Consistent Layout: The plot dimensions, grid styling, and typography match the updated formatting used in Step 1 and the anomaly visualisation.</li> <li>Legend Refinement: Cluster labels are clearly presented in the top-right legend, including an explicit <code>\"Noise\"</code> label to improve interpretability.</li> <li>Stable Colour Mapping: Colours are retained from the previous notebook (<code>orange</code>, <code>blue</code>, <code>green</code>, and <code>red</code>) to avoid re-learning visual cues.</li> <li>Extended X-Axis: The x-axis range is padded slightly to align with prior visualisations, improving symmetry across all plots.</li> <li>Clarity in PCA Projection: Plotting the PCA component over time continues to highlight subtle behavioural clusters, with noise points (<code>-1</code>) now more clearly delineated.</li> </ul> <p>While the underlying data remains unchanged, this refined presentation improves the professional readability of the plot, making it more suitable for teaching, peer review, or publication.</p>"},{"location":"06_visual_polishing_and_citations/#further-exploration-inspecting-cluster-substructure","title":"Further Exploration: Inspecting Cluster Substructure\u00b6","text":"<p>While the main plot presents all HDBSCAN clusters across the time series, some minor groups such as Cluster 2 (green) may not be immediately visible due to their low density and overlap with dominant patterns.</p> <p>Rather than separating these clusters into individual subplots, learners are encouraged to:</p> <ul> <li>Zoom in on local segments of the signal using slicing techniques (e.g. <code>df[df['timestamp'] &lt; 1000]</code>)</li> <li>Apply conditional filters (e.g. <code>df[df['cluster'] == 2]</code>) to investigate the specific timestamps and PCA values assigned to these smaller clusters</li> <li>Experiment with different <code>min_cluster_size</code> values in the HDBSCAN algorithm to observe how clustering granularity affects visibility and noise labelling</li> </ul> <p>This optional task develops close inspection and critical reasoning skills, supporting a deeper understanding of how structural patterns emerge within behavioural time series data.</p>"},{"location":"06_visual_polishing_and_citations/#citations","title":"Citations\u00b6","text":"<ul> <li>Campello, R.J.G.B., Moulavi, D. and Sander, J., 2013. Density-based clustering based on hierarchical density estimates. In: Pei, J., Tseng, V.S., Cao, L., Motoda, H. and Xu, G. (eds.) Advances in Knowledge Discovery and Data Mining. Lecture Notes in Computer Science, vol. 7819. Berlin: Springer, pp.160-172.</li> <li>Liu, F.T., Ting, K.M. and Zhou, Z.H., 2008. Isolation Forest. In: 2008 Eighth IEEE International Conference on Data Mining. Pisa, Italy, 15-19 December 2008. IEEE, pp.413-422.</li> <li>Dau, H.A., Bagnall, A., Kamgar, K., Yeh, C.C.M., Zhu, Y., Gharghabi, S., Ratanamahatana, C.A. and Keogh, E., 2019. The UCR time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6), pp.1293-1305.</li> </ul>"},{"location":"06_visual_polishing_and_citations/#visual-refinement-exercise-anomaly-plotting-on-the-fantasia-dataset","title":"Visual Refinement Exercise: Anomaly Plotting on the Fantasia Dataset\u00b6","text":"<p>This exercise invites you to practise visual polishing and scientific referencing using the <code>209Fantasia_clean.csv</code> dataset.</p> <p>Having already explored detection and clustering, your task is to now revisit Fantasia from a presentation and reproducibility perspective. Apply the plotting conventions shown earlier (e.g., signal with anomaly overlays), but enhance clarity and visual appeal through:</p> <ul> <li>Clear axis labels, titles, and colour choice</li> <li>Meaningful legends for anomaly markers</li> <li>Consistent scaling and layout</li> <li>Use of <code>matplotlib</code>, <code>seaborn</code>, or your preferred library for professional visualisation</li> </ul> <p>Citation Reminder: When plotting results from Fantasia, include a caption or note referencing the dataset\u2019s origin:</p> <p>\u201cFantasia dataset derived from the UCR Time Series Anomaly Archive (2021) [Keogh et al., SIGKDD 2021].\u201d</p> <p>Reflection prompts:</p> <ul> <li>How does the Fantasia signal lend itself to smooth visual presentation?</li> <li>What visual techniques best highlight subtle anomalies in periodic data?</li> <li>Does your citation clarify both the method and data source used?</li> </ul> <p>This exercise reinforces both the aesthetic and scholarly responsibilities of working with anomaly detection; teaching not just how to model, but how to communicate and credit.</p>"},{"location":"07_reproducibility_and_environment_testing/","title":"07 Reproducibility and Environment","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nimport platform\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport hdbscan\nimport matplotlib.pyplot as plt\nimport importlib.metadata\nimport warnings\n\nprint(\"Python version:\", sys.version)\nprint(\"Platform:\", platform.platform())\nprint(\"numpy version:\", np.__version__)\nprint(\"pandas version:\", pd.__version__)\nprint(\"matplotlib version:\", matplotlib.__version__)\nprint(\"scikit-learn version:\", sklearn.__version__)\n\ntry:\n    hdbscan_version = importlib.metadata.version(\"hdbscan\")\n    print(\"hdbscan version:\", hdbscan_version)\nexcept importlib.metadata.PackageNotFoundError:\n    print(\"hdbscan version: not found\")\nexcept Exception as e:\n    print(\"hdbscan version: error occurred \u2013\", str(e))\n</pre> import sys import platform import sklearn import numpy as np import pandas as pd import matplotlib import hdbscan import matplotlib.pyplot as plt import importlib.metadata import warnings  print(\"Python version:\", sys.version) print(\"Platform:\", platform.platform()) print(\"numpy version:\", np.__version__) print(\"pandas version:\", pd.__version__) print(\"matplotlib version:\", matplotlib.__version__) print(\"scikit-learn version:\", sklearn.__version__)  try:     hdbscan_version = importlib.metadata.version(\"hdbscan\")     print(\"hdbscan version:\", hdbscan_version) except importlib.metadata.PackageNotFoundError:     print(\"hdbscan version: not found\") except Exception as e:     print(\"hdbscan version: error occurred \u2013\", str(e))  <pre>Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPlatform: Linux-6.1.123+-x86_64-with-glibc2.35\nnumpy version: 2.0.2\npandas version: 2.2.2\nmatplotlib version: 3.10.0\nscikit-learn version: 1.6.1\nhdbscan version: 0.8.40\n</pre> In\u00a0[\u00a0]: Copied! <pre>import random\n\nRANDOM_STATE = 42\nrandom.seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs\n</pre> import random  RANDOM_STATE = 42 random.seed(RANDOM_STATE) np.random.seed(RANDOM_STATE)  from sklearn.ensemble import IsolationForest from sklearn.decomposition import PCA from sklearn.datasets import make_blobs  In\u00a0[\u00a0]: Copied! <pre>X, _ = make_blobs(n_samples=200, centers=1, cluster_std=0.60, random_state=RANDOM_STATE)\nX[:10] += np.random.normal(0, 3, size=X[:10].shape)  # Add anomalies manually\n\nmodel_a = IsolationForest(random_state=RANDOM_STATE)\nmodel_a.fit(X)\nscores_a = model_a.decision_function(X)\nlabels_a = model_a.predict(X)\n</pre> X, _ = make_blobs(n_samples=200, centers=1, cluster_std=0.60, random_state=RANDOM_STATE) X[:10] += np.random.normal(0, 3, size=X[:10].shape)  # Add anomalies manually  model_a = IsolationForest(random_state=RANDOM_STATE) model_a.fit(X) scores_a = model_a.decision_function(X) labels_a = model_a.predict(X) <p>Rerunning Isolation Forest with the Same Random Seed</p> <p>To demonstrate how seeding ensures reproducibility, we rerun the Isolation Forest model using the same random state. By fixing the seed, we aim to produce identical anomaly scores and predictions, thus verifying that our results can be repeated reliably.</p> <p>This step mirrors good practice in computational science, where reproducibility is regarded as the minimum threshold for trust in model outputs (Peng, 2011).</p> In\u00a0[\u00a0]: Copied! <pre>model_b = IsolationForest(random_state=RANDOM_STATE)\nmodel_b.fit(X)\nscores_b = model_b.decision_function(X)\nlabels_b = model_b.predict(X)\n</pre> model_b = IsolationForest(random_state=RANDOM_STATE) model_b.fit(X) scores_b = model_b.decision_function(X) labels_b = model_b.predict(X) <p>Verifying Output Consistency</p> <p>Here, we compare the anomaly scores and prediction labels from both model runs. If seeding has been correctly implemented, both outputs should match exactly. This reinforces the concept that deterministic model behaviour is essential for scientific reproducibility and deployment in regulated environments.</p> <p>Any inconsistency, even if minor, could raise doubts about the robustness of our modelling process. Ensuring reproducibility is not merely a technical detail but a scientific responsibility.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Scores equal:\", np.allclose(scores_a, scores_b))\nprint(\"Labels equal:\", np.array_equal(labels_a, labels_b))\n\nplt.figure(figsize=(10, 4))\nplt.plot(scores_a, label='Run A')\nplt.plot(scores_b, linestyle='dashed', label='Run B')\nplt.title(\"Isolation Forest Decision Function \u2013 Reproducibility Check\")\nplt.legend()\nplt.show()\n</pre> print(\"Scores equal:\", np.allclose(scores_a, scores_b)) print(\"Labels equal:\", np.array_equal(labels_a, labels_b))  plt.figure(figsize=(10, 4)) plt.plot(scores_a, label='Run A') plt.plot(scores_b, linestyle='dashed', label='Run B') plt.title(\"Isolation Forest Decision Function \u2013 Reproducibility Check\") plt.legend() plt.show() <pre>Scores equal: True\nLabels equal: True\n</pre> <p>The output confirms that both the anomaly scores and predicted labels are identical across runs (<code>True</code> for both tests), validating the effectiveness of the seeding procedure. This demonstrates that Isolation Forest, when provided a fixed <code>random_state</code>, produces entirely deterministic results.</p> <p>Visually, the plot reveals perfect overlap between the two runs, with Run B (dashed line) tracing precisely over Run A. This reinforces the model\u2019s reproducibility under controlled conditions and exemplifies good scientific practice, where identical code yields identical results.</p> <p>Such reproducibility is essential not only for model trust but also for operational robustness when deploying unsupervised anomaly detection in behavioural or safety-critical environments.</p> <p>To make clustering outputs more interpretable, we reduce our data to two dimensions using Principal Component Analysis (PCA). PCA is a deterministic method, meaning that if no randomness is introduced, it will consistently produce the same transformed values for a given input.</p> <p>Reducing to 2D enables us to visualise the output of the HDBSCAN clustering algorithm clearly. While the clustering itself is unsupervised, ensuring the input remains stable allows us to assess whether the algorithm behaves consistently across runs.</p> <p>We also suppress <code>FutureWarning</code> messages during execution to maintain a clean output, particularly when using <code>scikit-learn</code> utilities that may change in future versions.</p> In\u00a0[\u00a0]: Copied! <pre>pca = PCA(n_components=2)\nX_2d = pca.fit_transform(X)\n\nimport warnings\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n\n    clusterer_a = hdbscan.HDBSCAN(min_cluster_size=5)\n    labels_hdb_a = clusterer_a.fit_predict(X_2d)\n\n    clusterer_b = hdbscan.HDBSCAN(min_cluster_size=5)\n    labels_hdb_b = clusterer_b.fit_predict(X_2d)\n\nprint(\"HDBSCAN labels identical:\", np.array_equal(labels_hdb_a, labels_hdb_b))\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_hdb_a, cmap='viridis')\naxs[0].set_title(\"HDBSCAN Labels \u2013 Run A\")\naxs[1].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_hdb_b, cmap='viridis')\naxs[1].set_title(\"HDBSCAN Labels \u2013 Run B\")\nplt.show()\n</pre> pca = PCA(n_components=2) X_2d = pca.fit_transform(X)  import warnings with warnings.catch_warnings():     warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")      clusterer_a = hdbscan.HDBSCAN(min_cluster_size=5)     labels_hdb_a = clusterer_a.fit_predict(X_2d)      clusterer_b = hdbscan.HDBSCAN(min_cluster_size=5)     labels_hdb_b = clusterer_b.fit_predict(X_2d)  print(\"HDBSCAN labels identical:\", np.array_equal(labels_hdb_a, labels_hdb_b))  fig, axs = plt.subplots(1, 2, figsize=(12, 5)) axs[0].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_hdb_a, cmap='viridis') axs[0].set_title(\"HDBSCAN Labels \u2013 Run A\") axs[1].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_hdb_b, cmap='viridis') axs[1].set_title(\"HDBSCAN Labels \u2013 Run B\") plt.show()  <pre>HDBSCAN labels identical: True\n</pre> <p>The output confirms that the cluster labels produced by HDBSCAN are identical across both runs (<code>True</code>). This reflects the deterministic nature of the algorithm when applied to the same data, without randomness introduced through preprocessing.</p> <p>Visually, the side-by-side scatter plots show identical cluster assignments. Each point retains its colour-coded cluster across both panels, and outliers remain consistently positioned. This visual stability supports the claim that HDBSCAN does not require explicit seeding, provided the input remains unchanged.</p> <p>These results reinforce the importance of stable inputs when evaluating reproducibility in unsupervised clustering. In real-world behavioural systems, such consistency supports reliable grouping and downstream decision-making.</p> <p>To ensure that others can recreate our environment exactly, it is good practice to export the full list of package dependencies. This step allows collaborators, reviewers, or future users to reproduce the computational setup down to the library version.</p> <p>If we are using pip, we can generate a plain text file listing all installed packages:</p> In\u00a0[\u00a0]: Copied! <pre># !pip freeze &gt; requirements.txt\n</pre> # !pip freeze &gt; requirements.txt <p>Alternatively, if our workflow uses conda, we may prefer to export the full environment specification as a .yml file:</p> In\u00a0[\u00a0]: Copied! <pre># !conda env export &gt; environment.yml\n</pre> # !conda env export &gt; environment.yml"},{"location":"07_reproducibility_and_environment_testing/#07-reproducibility-and-environment","title":"07 Reproducibility and Environment\u00b6","text":"<p>This notebook ensures that the behavioural anomaly detection workflow developed in this exemplar is reproducible, robust, and shareable. Reproducibility is essential in scientific computing, particularly in high-stakes contexts such as behavioural monitoring, fraud detection, or security, where variation in model outputs due to environment changes could undermine trust.</p> <p>In this notebook, we use a synthetic dataset to illustrate how reproducibility can be enforced through seed control, stable libraries, and output verification. The principles are transferable, but for full reproducibility using the clinical data, one would re-run the earlier pipeline using <code>InternalBleeding14.csv</code> under logged conditions.</p>"},{"location":"07_reproducibility_and_environment_testing/#step-1-notebook-setup","title":"Step 1 - Notebook Setup\u00b6","text":"<p>We begin by importing the core libraries already used throughout the exemplar. These support data visualisation, anomaly scoring, and clustering tasks. Ensuring reproducibility begins with documenting the exact computational environment, including version numbers for Python and all key packages.</p> <p>Rather than relying on built-in attributes (which may be missing), the version of <code>hdbscan</code> is extracted using the <code>importlib.metadata</code> module. This approach is compatible with Python 3.8 and above, and is the recommended method for packages that do not expose a <code>__version__</code> attribute.</p> <p>For clarity and professional practice, all code comments about technical adjustments are instead explained in this markdown cell. We have also imported the <code>warnings</code> module to suppress <code>FutureWarning</code> messages from <code>scikit-learn</code>, which might otherwise distract from the reproducibility focus of this notebook.</p> <p>Learners may optionally run !pip list | tee environment_list.txt to export a full list of dependencies for future replication or publication.</p>"},{"location":"07_reproducibility_and_environment_testing/#step-2-set-global-random-seed-for-reproducibility","title":"Step 2 - Set Global Random Seed for Reproducibility\u00b6","text":"<p>To ensure consistent model behaviour across runs, we fix the random seed for both Python's built-in random module and NumPy. This practice stabilises random processes such as subsampling or noise generation, which are critical to algorithms like Isolation Forest.</p> <p>We also import several components used in subsequent sections. These include:</p> <ul> <li><code>make_blobs</code> for generating a synthetic dataset</li> <li><code>PCA</code> for dimensionality reduction</li> <li><code>IsolationForest</code> for unsupervised anomaly scoring</li> </ul> <p>By doing this early in the notebook, we ensure that learners experience consistent results and that any subsequent randomness is traceable to the fixed seed. Without this setup, the same notebook could produce slightly different outputs on each execution, which undermines reproducibility.</p>"},{"location":"07_reproducibility_and_environment_testing/#step-3-reproducibility-in-action-isolation-forest","title":"Step 3 - Reproducibility in Action - Isolation Forest\u00b6","text":"<p>Applying Isolation Forest for Anomaly Scoring</p> <p>We now apply the Isolation Forest algorithm to our synthetic dataset in order to generate anomaly scores. This model operates by recursively partitioning the data using random splits and measuring how quickly a point is isolated. The shorter the path to isolation, the more anomalous a point is considered.</p> <p>Isolation Forest is particularly well suited to behavioural anomaly detection, as it makes no assumptions about the data distribution. However, its stochastic nature makes it sensitive to randomness, which means reproducibility must be handled explicitly.</p>"},{"location":"07_reproducibility_and_environment_testing/#step-4-reproducibility-hdbscan-clustering","title":"Step 4 - Reproducibility HDBSCAN Clustering\u00b6","text":""},{"location":"07_reproducibility_and_environment_testing/#step-5-export-reproducibility-files","title":"Step 5 - Export Reproducibility Files\u00b6","text":""},{"location":"07_reproducibility_and_environment_testing/#reflection-and-summary","title":"Reflection and Summary\u00b6","text":"<p>Reflection Questions for Learners:</p> <ol> <li>Why is setting a random seed important in unsupervised anomaly detection?</li> <li>If your model gives different anomaly scores on a colleague's machine, how would you debug it?</li> <li>How might changing the version of <code>scikit-learn</code> or <code>numpy</code> affect reproducibility?</li> <li>What steps can be taken to ensure a model behaves identically during deployment?</li> <li>Why does HDBSCAN yield the same result even when rerun\u2014what makes it deterministic?</li> </ol> <p>Optional Task: Try rerunning HDBSCAN on data with slight perturbations. Observe how sensitive the clusters are.</p> <p>In summary this notebook has demonstrated how to:</p> <ul> <li>Log your environment for reproducibility.</li> <li>Set seeds to stabilise randomness in models.</li> <li>Confirm output consistency across reruns.</li> <li>Export your environment for future re-use.</li> <li>Validate reproducibility in clustering with HDBSCAN.</li> </ul> <p>These steps are crucial in scientific computing, enabling trust and repeatability\u2014both within your team and by external reviewers.</p>"},{"location":"07_reproducibility_and_environment_testing/#further-reading-and-references","title":"Further Reading and References\u00b6","text":"<ul> <li><p>Peng, R.D., 2011. Reproducible research in computational science. Science, 334(6060), pp.1226\u20131227.</p> </li> <li><p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E., 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, pp.2825\u20132830.</p> </li> <li><p>Gr\u00fcning, B., Chilton, J., K\u00f6ster, J., Dale, R., Soranzo, N., van den Beek, M., Goecks, J., Backofen, R., Nekrutenko, A. and Taylor, J., 2018. Practical computational reproducibility in the life sciences. Cell Systems, 6(6), pp.631\u2013635.</p> </li> </ul>"},{"location":"08_finalised_summary_notebook/","title":"08 Finalised Summary Notebook","text":""},{"location":"08_finalised_summary_notebook/#08-finalised-summary-notebook","title":"08 Finalised Summary Notebook\u00b6","text":"<p>This final notebook provides a high-level summary of the full pipeline developed across the ReCoDE exemplar. It is designed to help learners and reviewers grasp the structure, logic, and educational intent of the project.</p> <p>All core methods are revisited here, with emphasis on how each contributes to behavioural anomaly detection under data-scarce conditions.</p>"},{"location":"08_finalised_summary_notebook/#notebook-overview-pipeline-summary","title":"Notebook Overview (Pipeline Summary)\u00b6","text":"Notebook Title Purpose <code>01</code> Dataset Preparation Introduces the cleaned behavioural dataset and prepares it for modelling <code>02</code> Preprocessing and Baseline IForest Applies preprocessing steps and establishes a baseline using Isolation Forest <code>03</code> Dimensionality and Clustering Reduces feature space using PCA and applies HDBSCAN for behavioural segmentation <code>04</code> Model Interpretation and Explanation Interprets anomaly outputs and visualises key patterns <code>05</code> Ethical Reflection and False Positives Highlights key ethical risks, including false positives and model opacity <code>06</code> Visual Polishing and Citations Refines visual presentation and links methods to academic sources <code>07</code> Reproducibility and Environment Testing Reruns key components under deterministic conditions to verify reproducibility"},{"location":"08_finalised_summary_notebook/#final-pipeline-flow","title":"Final Pipeline Flow\u00b6","text":"<pre><code>Raw Behavioural Data  \n   \u2192 Preprocessing and Preparation  \n       \u2192 PCA for Dimensionality Reduction  \n           \u2192 HDBSCAN for Behavioural Segmentation  \n               \u2192 Isolation Forest for Anomaly Detection  \n                   \u2192 Visual Review and Ethical Reflection  \n                       \u2192 Reproducibility Validation\n\n</code></pre>"},{"location":"08_finalised_summary_notebook/#key-methodological-choices","title":"Key Methodological Choices\u00b6","text":"<ul> <li>Unsupervised Learning: Isolation Forest, HDBSCAN</li> <li>Dimensionality Reduction: PCA</li> <li>Cluster Validation: Behaviour-based segmentation</li> <li>Ethics &amp; Fairness: Scenario-based discussion of false positives</li> <li>Reproducibility: Fixed seeds, controlled library versions</li> </ul>"},{"location":"08_finalised_summary_notebook/#recode-learning-goals-addressed","title":"ReCoDE Learning Goals Addressed\u00b6","text":"<ul> <li><p>Working with incomplete behavioural data</p> </li> <li><p>Applying unsupervised techniques to detect behavioural  irregularities in ambiguous or label-scarce contexts</p> </li> <li><p>Using interpretable models under ethical constraints</p> </li> <li><p>Ensuring code reproducibility and transparency</p> </li> </ul>"},{"location":"08_finalised_summary_notebook/#next-steps-and-encouragement","title":"Next Steps and Encouragement\u00b6","text":"<p>Learners are invited to experiment with:</p> <ul> <li>Different clustering settings (e.g., minimum cluster size)</li> <li>Anomaly detection thresholds</li> <li>Feature inclusion/exclusion</li> <li>Ethical questions in applied settings</li> </ul> <p>For further details, see the main README or revisit the annotated notebooks.</p>"}]}